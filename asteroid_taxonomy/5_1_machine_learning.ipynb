{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f14c9d7a",
   "metadata": {},
   "source": [
    "# Step 5.1: Machine Learning - A first step\n",
    "\n",
    "We are now set to perform some Machine Learning on the Asteroid Spectra Data! We keep it simple though: The multiclass clssificaiton problem of the Main Group classes:\n",
    "\n",
    "- C\n",
    "- S\n",
    "- X\n",
    "- Other\n",
    "\n",
    "... shall be transformed into a binary problem. E.g.: X (1) and Not-X (0). In this script we use a Support Vector Machine (SVM) algorithm to perform some classification tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63474560",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import standard libraries\n",
    "import os\n",
    "\n",
    "# Import installed libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af9f8681",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's mount the Google Drive, where we store files and models (if applicable, otherwise work\n",
    "# locally)\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/gdrive')\n",
    "    core_path = \"/gdrive/MyDrive/Colab/asteroid_taxonomy/\"\n",
    "except ModuleNotFoundError:\n",
    "    core_path = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71bc69f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the level 2 asteroid data\n",
    "asteroids_df = pd.read_pickle(os.path.join(core_path, \"data/lvl2/\", \"asteroids.pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36f27a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we add a binary classification schema, where we distinguish between e.g., X and non-X classes\n",
    "asteroids_df.loc[:, \"Class\"] = asteroids_df[\"Main_Group\"].apply(lambda x: 1 if x==\"X\" else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62493261",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allocate the spectra to one array and the classes to another one\n",
    "asteroids_X = np.array([k[\"Reflectance_norm550nm\"].tolist() for k in asteroids_df[\"SpectrumDF\"]])\n",
    "asteroids_y = np.array(asteroids_df[\"Class\"].to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4572a12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this example we create a single test-training split with a ratio of 0.8 / 0.2\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=0)\n",
    "\n",
    "for train_index, test_index in sss.split(asteroids_X, asteroids_y):\n",
    "    X_train, X_test = asteroids_X[train_index], asteroids_X[test_index]\n",
    "    y_train, y_test = asteroids_y[train_index], asteroids_y[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c26c24a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ration of positive training classes: 0.18\n",
      "Ration of positive test classes: 0.18\n"
     ]
    }
   ],
   "source": [
    "# Let's take a look whether the unbalanced ratio has been preserved\n",
    "print(f\"Ration of positive training classes: {round(sum(y_train) / len(X_train), 2)}\")\n",
    "print(f\"Ration of positive test classes: {round(sum(y_test) / len(X_test), 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd69d7fd",
   "metadata": {},
   "source": [
    "## Unbalaned Datasets\n",
    "The inverse of the ratio is approximately 5. We need this, to weight our unbalanced training set during the ML fitting process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "47298888",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive Class weightning: 5\n"
     ]
    }
   ],
   "source": [
    "# Compute class weightning\n",
    "positive_class_weight = int(1.0 / (sum(y_train) / len(X_train)))\n",
    "print(f\"Positive Class weightning: {positive_class_weight}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab5b34f",
   "metadata": {},
   "source": [
    "## Scaling\n",
    "\n",
    "Scaling the training and test data should be done AFTER the splitting to prevent data leakage from the test data to the training data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd171dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the preprocessing module\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# Instantiate the StandardScaler (mean 0, standard deviation 1) and use the training data to fit\n",
    "# the scaler\n",
    "scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "\n",
    "# Transform now the training data\n",
    "X_train_scaled = scaler.transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee32a9a",
   "metadata": {},
   "source": [
    "## The training\n",
    "\n",
    "Please note: This training is rather simple; it does not use multiple training / test splits, performs no logging and performs no GridSearch to find the best set of parameters!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "66fa81c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=100, class_weight={1: 5})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the SVM class\n",
    "from sklearn import svm\n",
    "\n",
    "# Call the SVM class, use an RBF kernel and apply the class weightning. That's it!\n",
    "wclf = svm.SVC(kernel='rbf', class_weight={1: positive_class_weight}, C=100)\n",
    "\n",
    "# Perform the training\n",
    "wclf.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6985e960",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the testing data ...\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ... and perform a predicition\n",
    "y_test_pred = wclf.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde19b54",
   "metadata": {},
   "source": [
    "## Metrics - a first look\n",
    "\n",
    "<i>What kind of metrics are useful to determine the performance of a classifier?</i>\n",
    "\n",
    "The answer to this questions depends on one's expectations on a classifiers behaviour: Is the \"overall\" performance important? Does one want to prevent false positives? Or false negatives? Is the number of positive results the most important metric?\n",
    "\n",
    "Fictional project scope: Let's assume that our final model shall later support scientists as a tool to find X Class spectra in a vast dataset. Further, let's assume that the scientists' task is to find as many X Class spectra as possible. Consequently the scientists do not want to have falsely classified X spectra. We take a look first at the confusion matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "97aa4039",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[217   4]\n",
      " [  2  45]]\n"
     ]
    }
   ],
   "source": [
    "# Import the confusion matrix and perform the computation\n",
    "from sklearn.metrics import confusion_matrix\n",
    "conf_mat = confusion_matrix(y_test, y_test_pred)\n",
    "\n",
    "print(conf_mat)\n",
    "\n",
    "# The order of the confusion matrix is:\n",
    "#     - true negative (top left, tn)\n",
    "#     - false positive (top right, fp)\n",
    "#     - false negative (bottom left, fn)\n",
    "#     - true positive (bottom right, tp)\n",
    "tn, fp, fn, tp = conf_mat.ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a40869",
   "metadata": {},
   "source": [
    "## Metrics - what matters\n",
    "\n",
    "Fictional project scope: The number of false negatives should be as small as possible (the scientist does not want to lose preciouse spectra!). Further, the number of false positives might be higher. The scientists will sort them out later on. A less strict requirement for the so called False Discovery Rate enables the Machine Learning Engineer to find a solution quicker in a broader solution space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5686e26e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall Score: 0.957\n",
      "Precision Score: 0.918\n",
      "F1 Score: 0.938\n"
     ]
    }
   ],
   "source": [
    "# Recall: ratio of correctly classified X Class spectra, considering the false negatives\n",
    "# (recall = tp / (tp + fn))\n",
    "recall_score = round(sklearn.metrics.recall_score(y_test, y_test_pred), 3)\n",
    "print(f\"Recall Score: {recall_score}\")\n",
    "\n",
    "# Precision: ratio of correctly classified X Class spectra, considering the false positives\n",
    "# (precision = tp / (tp + fp))\n",
    "precision_score = round(sklearn.metrics.precision_score(y_test, y_test_pred), 3)\n",
    "print(f\"Precision Score: {precision_score}\")\n",
    "\n",
    "# A combined score\n",
    "f1_score = round(sklearn.metrics.f1_score(y_test, y_test_pred), 3)\n",
    "print(f\"F1 Score: {f1_score}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
