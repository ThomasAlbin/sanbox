{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f14c9d7a",
   "metadata": {},
   "source": [
    "# Step 5.: Machine Learning - Parameter Search / Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63474560",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import standard libraries\n",
    "import os\n",
    "\n",
    "# Import installed libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af9f8681",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's mount the Google Drive, where we store files and models (if applicable, otherwise work\n",
    "# locally)\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/gdrive')\n",
    "    core_path = \"/gdrive/MyDrive/Colab/asteroid_taxonomy/\"\n",
    "except ModuleNotFoundError:\n",
    "    core_path = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71bc69f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the level 2 asteroid data\n",
    "asteroids_df = pd.read_pickle(os.path.join(core_path, \"data/lvl2/\", \"asteroids.pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36f27a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we add a binary classification schema, where we distinguish between e.g., X and non-X classes\n",
    "asteroids_df.loc[:, \"Class\"] = asteroids_df[\"Main_Group\"].apply(lambda x: 1 if x==\"X\" else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62493261",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allocate the spectra to one array and the classes to another one\n",
    "asteroids_X = np.array([k[\"Reflectance_norm550nm\"].tolist() for k in asteroids_df[\"SpectrumDF\"]])\n",
    "asteroids_y = np.array(asteroids_df[\"Class\"].to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "169800d4-82a2-4137-b00e-41dc437ce1c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here\n",
      "[LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM]"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'stop' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-c850a9658428>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mwclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_scaled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m     \u001b[0mstop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'stop' is not defined"
     ]
    }
   ],
   "source": [
    "# In this example we create a single test-training split with a ratio of 0.8 / 0.2\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "sss = StratifiedShuffleSplit(n_splits=11, test_size=0.2, random_state=0)\n",
    "\n",
    "\n",
    "\n",
    "result_df = pd.DataFrame([], columns=[])\n",
    "\n",
    "for train_index, test_index in sss.split(asteroids_X, asteroids_y):\n",
    "    \n",
    "    temp_results = {}\n",
    "    \n",
    "    X_train, X_test = asteroids_X[train_index], asteroids_X[test_index]\n",
    "    y_train, y_test = asteroids_y[train_index], asteroids_y[test_index]\n",
    "    \n",
    "    # Compute class weightning\n",
    "    positive_class_weight = int(1.0 / (sum(y_train) / len(X_train)))\n",
    "\n",
    "    parameters = {'kernel':('linear', 'rbf', 'poly'),\n",
    "                  'C':[0.1, 1, 10, 100, 1000]}\n",
    "    svc = svm.SVC(class_weight={1: positive_class_weight}, verbose=True)\n",
    "    \n",
    "    # Import the preprocessing module\n",
    "\n",
    "    # Instantiate the StandardScaler (mean 0, standard deviation 1) and use the training data to fit\n",
    "    # the scaler\n",
    "    scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "\n",
    "    # Transform now the training data\n",
    "    X_train_scaled = scaler.transform(X_train)\n",
    "    print(\"Here\")\n",
    "    wclf = GridSearchCV(svc, parameters, scoring='recall')\n",
    "    \n",
    "    # Perform the training\n",
    "    wclf.fit(X_train_scaled, y_train)\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3aeba75-c7a6-4718-a2d6-27ec0da5f069",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([2.50409093, 0.00687008]),\n",
       " 'std_fit_time': array([0.84438061, 0.001063  ]),\n",
       " 'mean_score_time': array([0.00340405, 0.00364099]),\n",
       " 'std_score_time': array([0.00025272, 0.00030883]),\n",
       " 'param_C': masked_array(data=[100, 100],\n",
       "              mask=[False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_kernel': masked_array(data=['linear', 'rbf'],\n",
       "              mask=[False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'C': 100, 'kernel': 'linear'}, {'C': 100, 'kernel': 'rbf'}],\n",
       " 'split0_test_score': array([0.86842105, 0.94736842]),\n",
       " 'split1_test_score': array([0.84210526, 0.92105263]),\n",
       " 'split2_test_score': array([0.94736842, 0.94736842]),\n",
       " 'split3_test_score': array([0.89473684, 0.94736842]),\n",
       " 'split4_test_score': array([0.89473684, 0.97368421]),\n",
       " 'mean_test_score': array([0.88947368, 0.94736842]),\n",
       " 'std_test_score': array([0.03491184, 0.01664357]),\n",
       " 'rank_test_score': array([2, 1], dtype=int32)}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wclf.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6985e960",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the testing data ...\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ... and perform a predicition\n",
    "y_test_pred = wclf.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97aa4039",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the confusion matrix and perform the computation\n",
    "from sklearn.metrics import confusion_matrix\n",
    "conf_mat = confusion_matrix(y_test, y_test_pred)\n",
    "\n",
    "print(conf_mat)\n",
    "\n",
    "# The order of the confusion matrix is:\n",
    "#     - true negative (top left, tn)\n",
    "#     - false positive (top right, fp)\n",
    "#     - false negative (bottom left, fn)\n",
    "#     - true positive (bottom right, tp)\n",
    "tn, fp, fn, tp = conf_mat.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5686e26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recall: ratio of correctly classified X Class spectra, considering the false negatives\n",
    "# (recall = tp / (tp + fn))\n",
    "recall_score = round(sklearn.metrics.recall_score(y_test, y_test_pred), 3)\n",
    "print(f\"Recall Score: {recall_score}\")\n",
    "\n",
    "# Precision: ratio of correctly classified X Class spectra, considering the false positives\n",
    "# (precision = tp / (tp + fp))\n",
    "precision_score = round(sklearn.metrics.precision_score(y_test, y_test_pred), 3)\n",
    "print(f\"Precision Score: {precision_score}\")\n",
    "\n",
    "# A combined score\n",
    "f1_score = round(sklearn.metrics.f1_score(y_test, y_test_pred), 3)\n",
    "print(f\"F1 Score: {f1_score}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
