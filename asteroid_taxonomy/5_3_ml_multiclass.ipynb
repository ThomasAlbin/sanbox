{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f14c9d7a",
   "metadata": {},
   "source": [
    "# Step 5.3: Machine Learning - Multiclass\n",
    "\n",
    "Finally, we focus now on a Halving Grid Search method with a multiclass SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63474560",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import standard libraries\n",
    "import os\n",
    "\n",
    "# Import installed libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af9f8681",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's mount the Google Drive, where we store files and models (if applicable, otherwise work\n",
    "# locally)\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/gdrive')\n",
    "    core_path = \"/gdrive/MyDrive/Colab/asteroid_taxonomy/\"\n",
    "except ModuleNotFoundError:\n",
    "    core_path = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71bc69f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the level 2 asteroid data\n",
    "asteroids_df = pd.read_pickle(os.path.join(core_path, \"data/lvl2/\", \"asteroids.pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62493261",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allocate the spectra to one array and the classes to another one\n",
    "asteroids_X = np.array([k[\"Reflectance_norm550nm\"].tolist() for k in asteroids_df[\"SpectrumDF\"]])\n",
    "asteroids_y = np.array(asteroids_df[\"Main_Group\"].to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "39ead307-7368-4991-925d-9daba407e2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this example we create a single test-training split with a ratio of 0.8 / 0.2\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2)\n",
    "\n",
    "# Create a simple, single train / test split\n",
    "for train_index, test_index in sss.split(asteroids_X, asteroids_y):\n",
    "    \n",
    "    X_train, X_test = asteroids_X[train_index], asteroids_X[test_index]\n",
    "    y_train, y_test = asteroids_y[train_index], asteroids_y[test_index]\n",
    "\n",
    "# Compute class weightnings\n",
    "weight_dict = {}\n",
    "for ast_type in np.unique(y_train):\n",
    "    weight_dict[ast_type] = int(1.0 / (len(y_train[y_train == ast_type]) / (len(y_train))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "87548b42-2b1f-4f2b-8718-600394d91b8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "[CV 1/5] END ...........................C=100, kernel=linear; total time=   3.8s\n",
      "[CV 2/5] END ...........................C=100, kernel=linear; total time=   3.6s\n",
      "[CV 3/5] END ...........................C=100, kernel=linear; total time=   3.0s\n",
      "[CV 4/5] END ...........................C=100, kernel=linear; total time=   2.7s\n",
      "[CV 5/5] END ...........................C=100, kernel=linear; total time=   4.8s\n",
      "[CV 1/5] END ..............................C=100, kernel=rbf; total time=   0.0s\n",
      "[CV 2/5] END ..............................C=100, kernel=rbf; total time=   0.0s\n",
      "[CV 3/5] END ..............................C=100, kernel=rbf; total time=   0.0s\n",
      "[CV 4/5] END ..............................C=100, kernel=rbf; total time=   0.0s\n",
      "[CV 5/5] END ..............................C=100, kernel=rbf; total time=   0.0s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=SVC(class_weight={'C': 3, 'Other': 8, 'S': 2, 'X': 5}),\n",
       "             param_grid=[{'C': [100], 'kernel': ['linear']},\n",
       "                         {'C': [100], 'kernel': ['rbf']}],\n",
       "             scoring=make_scorer(f1_score, average=weighted), verbose=3)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import make_scorer, f1_score\n",
    "\n",
    "# Perform now a GridSearch with the following parameter range and kernels\n",
    "param_grid = [\n",
    "  {'C': [100], 'kernel': ['linear']},\n",
    "  {'C': [100], 'kernel': ['rbf']},\n",
    " ]\n",
    "\n",
    "# Set the SVM classifier\n",
    "svc = svm.SVC(class_weight=weight_dict)\n",
    "\n",
    "# Instantiate the StandardScaler (mean 0, standard deviation 1) and use the training data to fit\n",
    "# the scaler\n",
    "scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "\n",
    "# Transform now the training data\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "\n",
    "# Set the GridSearch and ...\n",
    "wclf = GridSearchCV(svc, param_grid, scoring=make_scorer(f1_score, average=\"weighted\"), verbose=3, cv=5)\n",
    "\n",
    "# ... perform the training!\n",
    "wclf.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "16c0d08f-5dea-4595-9041-30201d1a91f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: get the best estimator\n",
    "final_clf = wclf.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3b4f287e-398e-44ff-befe-6c6bd777d22f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=100, class_weight={'C': 3, 'Other': 8, 'S': 2, 'X': 5})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6985e960",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the testing data ...\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ... and perform a predicition\n",
    "y_test_pred = final_clf.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "06b35930-9224-4fbb-a45b-1b0fc7cd8fac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['C', 'S', 'X', 'X', 'S', 'X', 'S', 'C', 'S', 'S', 'C', 'S', 'C',\n",
       "       'S', 'S', 'X', 'Other', 'C', 'S', 'S', 'X', 'S', 'Other', 'S', 'C',\n",
       "       'S', 'X', 'S', 'S', 'C', 'X', 'X', 'C', 'S', 'S', 'Other', 'X',\n",
       "       'S', 'C', 'X', 'C', 'X', 'S', 'X', 'S', 'X', 'S', 'S', 'Other',\n",
       "       'Other', 'C', 'X', 'S', 'C', 'S', 'S', 'C', 'S', 'C', 'S', 'C',\n",
       "       'C', 'S', 'Other', 'C', 'C', 'C', 'Other', 'S', 'C', 'S', 'C',\n",
       "       'Other', 'S', 'C', 'Other', 'C', 'X', 'C', 'S', 'S', 'S', 'C', 'S',\n",
       "       'X', 'Other', 'S', 'S', 'C', 'S', 'X', 'S', 'C', 'C', 'Other', 'C',\n",
       "       'X', 'S', 'X', 'S', 'S', 'C', 'C', 'S', 'C', 'C', 'X', 'S', 'C',\n",
       "       'S', 'S', 'C', 'S', 'S', 'C', 'S', 'X', 'C', 'C', 'C', 'C',\n",
       "       'Other', 'S', 'S', 'S', 'C', 'S', 'C', 'S', 'Other', 'S', 'S', 'X',\n",
       "       'X', 'S', 'Other', 'Other', 'C', 'Other', 'S', 'C', 'C', 'S', 'X',\n",
       "       'X', 'S', 'Other', 'Other', 'X', 'S', 'Other', 'C', 'S', 'X', 'C',\n",
       "       'S', 'S', 'C', 'S', 'S', 'X', 'S', 'Other', 'S', 'S', 'C', 'S',\n",
       "       'S', 'X', 'S', 'C', 'C', 'X', 'C', 'S', 'S', 'C', 'Other', 'C',\n",
       "       'C', 'S', 'X', 'Other', 'S', 'S', 'Other', 'S', 'S', 'S', 'C',\n",
       "       'Other', 'C', 'C', 'S', 'S', 'S', 'X', 'X', 'X', 'S', 'X', 'S',\n",
       "       'X', 'C', 'X', 'Other', 'C', 'S', 'Other', 'C', 'X', 'S', 'S', 'S',\n",
       "       'C', 'C', 'S', 'X', 'C', 'X', 'S', 'S', 'X', 'S', 'S', 'X', 'X',\n",
       "       'X', 'C', 'S', 'X', 'S', 'C', 'S', 'S', 'C', 'S', 'S', 'C', 'C',\n",
       "       'X', 'Other', 'C', 'X', 'S', 'S', 'S', 'C', 'C', 'Other', 'X',\n",
       "       'Other', 'S', 'S', 'X', 'S', 'C', 'C', 'S', 'C', 'S', 'C', 'S',\n",
       "       'Other', 'X', 'S', 'S', 'S'], dtype='<U5')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "97aa4039",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 75   0   0   4]\n",
      " [  0  26   5   0]\n",
      " [  0   3 107   0]\n",
      " [  1   1   0  46]]\n"
     ]
    }
   ],
   "source": [
    "# Import the confusion matrix and perform the computation\n",
    "from sklearn.metrics import confusion_matrix\n",
    "conf_mat = confusion_matrix(y_test, y_test_pred)\n",
    "\n",
    "print(conf_mat)\n",
    "\n",
    "# The order of the confusion matrix is:\n",
    "#     - true negative (top left, tn)\n",
    "#     - false positive (top right, fp)\n",
    "#     - false negative (bottom left, fn)\n",
    "#     - true positive (bottom right, tp)\n",
    "#tn, fp, fn, tp = conf_mat.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5686e26e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.948\n"
     ]
    }
   ],
   "source": [
    "# A combined score\n",
    "f1_score = round(sklearn.metrics.f1_score(y_test, y_test_pred, average=\"weighted\"), 3)\n",
    "print(f\"F1 Score: {f1_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13b2fe6-1b77-4af6-b0f0-3fbd2f444243",
   "metadata": {},
   "source": [
    "# Summary / Outlook:\n",
    "\n",
    "To finalize: Apply all data (take care of scaling!) on the training and rerun it. Save it afterwards for further computations. Storing and using a model requires also to store the corresponding scaler (store them as pickle files)!\n",
    "\n",
    "Further notes:<br>\n",
    "- If you would like to improve the model later on, one needs to perform \"partial fits\" to improve / train the weight(s). A simple \"fit\" re-runs the training and computes the model from scratch. Only linear SVMs can be partially fitted using [SGDClassifiers](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html)\n",
    "- Using all data to train a final model requires a proper metric. Cross validation as shown above can be used, but one should consider \"Nested Cross Validation\" methods as shown [here](https://scikit-learn.org/stable/auto_examples/model_selection/plot_nested_cross_validation_iris.html)\n",
    "- Also: a computational more extensive method called [HalvingGridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.HalvingGridSearchCV.html#sklearn.model_selection.HalvingGridSearchCV) may improve the model even further"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
